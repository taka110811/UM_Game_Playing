Overview
In this competition, you’ll create a model to predict how well one Monte-Carlo tree search (MCTS) variant will do against another in a given game, based on a list of features describing the game. This challenge aims to help us figure out which MCTS variants work best in different types of games, so we can make more informed choices when applying these algorithms to new problems.

Start

a day ago
Close
3 months to go
Merger & Entry
Description
MCTS is a widely used search algorithm for developing agents that can play board games intelligently. Over the past two decades, researchers have proposed dozens, if not hundreds, of MCTS variants. Despite this, it's been challenging to determine which variants are best suited for specific types of games.

In most studies, researchers demonstrate that a new MCTS variant outperforms one or a few other variants in a limited set of games. However, it’s uncommon for a new variant to consistently outperform others across a broad range of games, making it unclear which types of games certain MCTS variants are best at. Answering this question would greatly improve our understanding of MCTS algorithms, and help us make better decisions about which variants to apply to new games or other decision-making problems.

This competition challenges you to develop a model that can predict the performance of one MCTS variant against another in a given game, based on the features of the game.

Your work could help pave the way for identifying the strengths and weaknesses of different MCTS variants, advancing our understanding of where they work best in various scenarios.

Evaluation
Submissions are evaluated based on the Root-mean-square-error (RMSE) between predicted and ground-truth performance levels of the first agent against the second agent.

Submitting
You must submit to this competition using the provided Python evaluation API, which serves test set instances in random order in batches of 100. To use the API, follow the template in this notebook.

Timeline
September 5, 2024 - Start Date.

November 25, 2024 - Entry Deadline. You must accept the competition rules before this date in order to compete.

November 25, 2024 - Team Merger Deadline. This is the last day participants may join or merge teams.

December 2, 2024 - Final Submission Deadline.

All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.

Prizes
1st Place - $20,000
2nd Place - $12,000
3rd Place - $7,000
4th Place - $6,000
5th Place - $5,000
Code Requirements
This is a Code Competition
Submissions to this competition must be made through Notebooks. In order for the "Submit" button to be active after a commit, the following conditions must be met:

CPU Notebook <= 9 hours run-time
GPU Notebook <= 9 hours run-time
Internet access disabled
Freely & publicly available external data is allowed, including pre-trained models
Submission file must be named submission.parquet and be generated by the evaluation API.
Each batch of predictions must be returned within in 10 minutes of the batch data being provided.
Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors.

Organizers & Contributors
Dennis J.N.J. Soemers, Department of Advanced Computing Sciences, Maastricht University, the Netherlands.
Éric Piette, ICTEAM, Université catholique de Louvain, Belgium.
Achille Morenville, ICTEAM, Université catholique de Louvain, Belgium.
Matthew Stephenson, College of Science and Engineering, Flinders University, Australia.
Kurt Driessens, Department of Advanced Computing Sciences, Maastricht University, the Netherlands.
Mark H.M. Winands, Department of Advanced Computing Sciences, Maastricht University, the Netherlands.
Acknowledgements
This competition is organized as a collaboration between Maastricht University, Université catholique de Louvain, Flinders University, and Kaggle.












UM - Game-Playing Strength of MCTS Variants | Kaggle

概要

このコンペティションでは、特定のゲームにおいて、あるモンテカルロ木探索（MCTS）バリアントが他のバリアントに対してどれだけ優れたパフォーマンスを発揮するかを予測するモデルを作成します。これは、異なるタイプのゲームにおいて、どのMCTSバリアントが最適であるかを判断するのに役立ちます。

開始

	•	1日前

締め切り

	•	3か月後

説明

MCTSは、ボードゲームでの知的なエージェント開発に広く使用されている探索アルゴリズムです。過去20年間で、数十、あるいは数百のMCTSバリアントが提案されてきましたが、特定のゲームに最も適したバリアントを特定することは困難です。

多くの研究で、新しいMCTSバリアントが一部のゲームで他のバリアントよりも優れていることが示されていますが、すべてのゲームで一貫して優れているとは限りません。これを明らかにすることで、MCTSアルゴリズムの理解が向上し、新しいゲームや意思決定問題に適用する際により良い判断ができるようになります。

このコンペティションでは、ゲームの特徴に基づいて、あるMCTSバリアントが他のバリアントに対してどれだけ優れたパフォーマンスを発揮するかを予測するモデルを開発します。

評価

提出物は、最初のエージェントと2つ目のエージェントの実際のパフォーマンスレベルと予測されたパフォーマンスレベルとの間の**二乗平均平方根誤差（RMSE）**に基づいて評価されます。

提出

このコンペティションに提出するには、提供されたPython評価APIを使用します。このAPIは、テストセットのインスタンスをランダムな順序で100個ずつ提供します。APIの使用方法はこのノートブックを参照してください。
